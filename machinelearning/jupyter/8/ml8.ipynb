{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 基于实例的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "简介\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "基于实例的学习（Instance Based Learning ）\n",
    "------------------------------------------\n",
    "\n",
    "-   $k$ -近邻 ( $k$ -Nearest Neighbor)\n",
    "-   局部加权回归 （Locally weighted regression）\n",
    "-   径向基函数（Radial basis functions）\n",
    "-   Case-based reasoning\n",
    "-   Lazy and eager learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "KNN\n",
    "===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "K最近邻算法\n",
    "-----------\n",
    "\n",
    "关键思想： 只保存所有训练样例 $\\langle x_i, f(x_i) \\rangle$\n",
    "\n",
    "-   最近邻\n",
    "    -   给定查询实例 $x_q$, 首选确定最近的训练实例 $x_n$,\n",
    "    -   然后估计 $\\hat{f}(x_q) \\leftarrow f(x_n)$\n",
    "-   $k$ - 最近邻\n",
    "    -   给定查询实例 Given $x_q$ , 最近的 $k$ 个训练实例投票\n",
    "        (目标函数为离散值)\n",
    "    -   最近的 $k$ 个训练样例的 $f$ 值取平均(实值目标函数)\n",
    "        $$\\hat{f}(x_{q}) \\leftarrow  \\frac{\\sum_{i=1}^{k}f(x_{i})}{k}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "何时使用最近邻算法\n",
    "------------------\n",
    "\n",
    "-   实例映身到空间 $\\Re^n$ 中的点\n",
    "\n",
    "-   每实例少于 20 个属性\n",
    "\n",
    "-   大量训练数据\n",
    "\n",
    "-   优点:\n",
    "\n",
    "    -   训练快\n",
    "    -   可学习复杂函数\n",
    "    -   不损失信息\n",
    "\n",
    "-   缺点:\n",
    "\n",
    "    -   查询慢\n",
    "    -   易受不相关属性干扰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Voronoi Diagram\n",
    "---------------\n",
    "\n",
    "![](./image/knn-f1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Behavior in the Limit\n",
    "---------------------\n",
    "\n",
    "设 $p(x)$ 定义为实例 $x$ 应被标记为 1 (positive) 而不是 0 (negative)\n",
    "的概率。\n",
    "\n",
    "最近邻:\n",
    "\n",
    "-   当训练样例数量 $\\rightarrow \\infty$, 逼近 Gibbs 算法\n",
    "-   Gibbs: 按概率 $p(x)$ 预测 1, 否则 0\n",
    "\n",
    "$k$ -最近邻:\n",
    "\n",
    "-   当训练样例数量 $\\rightarrow \\infty$ 且 $k$ 值较大,\n",
    "    逼近贝叶斯最优分类器\n",
    "-   Bayes optimal: 当 $p(x)>.5$ 预测 1, 否则 0\n",
    "\n",
    "注意： 期望错误率 Gibbs 至多是 Bayes optimal 的两倍。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "距离加权KNN算法（Distance-Weighted $k$ NN）\n",
    "-------------------------------------------\n",
    "\n",
    "-   也许想让距离近的权重大些\n",
    "    $$\\hat{f}(x_{q}) \\leftarrow  \\frac{\\sum_{i=1}^{k} w_{i} f(x_{i})}{\\sum_{i=1}^{k} w_{i}}$$\n",
    "    其中\n",
    "    -   $$w_{i} \\equiv \\frac{1}{d(x_{q}, x_{i})^{2}}$$\n",
    "    -   $d(x_{q}, x_{i})$ 是 $x_{q}$ 与 $x_{i}$ 之间的距离\n",
    "-   注意：现在它可以使用所有训练样例而不仅是 $k$ 个。(Shepard\\'s method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Curse of Dimensionality\n",
    "-----------------------\n",
    "\n",
    "-   考虑把k-近邻算法应用到这样一个问题：\n",
    "    -   每个实例由20个属性描述，但在这些属性中仅有2个与它的分类是有关。\n",
    "    -   在这种情况下，这两个相关属性的值一致的实例可能在这个20维的实例空间中相距很远。\n",
    "    -   结果，依赖这20个属性的相似性度量会误导k-近邻算法的分类。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Curse of Dimensionality\n",
    "-----------------------\n",
    "\n",
    "-   *Curse of dimensionality : 高维空间 $X$ 中最近邻易被误导*\n",
    "-   一种解决方法\n",
    "    -   按权重 $z_j$ 伸展第 $j$ 个坐标轴 , 其中 $z_1, \\ldots, z_n$\n",
    "        按最小预测误差选取\n",
    "    -   使用交叉验证（ cross-validation）自动选取权重 $z_1, \\ldots, z_n$\n",
    "    -   设置 $z_j$ 为0可消除第 $j$ 维影响\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "局部加权回归\n",
    "============\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "局部加权回归（Locally Weighted Regression）\n",
    "-------------------------------------------\n",
    "\n",
    "-   kNN 为每个查询点 $x_q$ 构造了 $f$ 的局部逼近\n",
    "-   局部加权回归为包含 $x_q$ 的区域显示地构造逼近函数 $\\hat{f}(x)$\n",
    "    -   对 $k$ 个近邻 拟合线性函数\n",
    "    -   拟合二次函数\n",
    "    -   分段逼近 $f$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "误差准则:\n",
    "---------\n",
    "\n",
    "-   $k$ 近邻的误差平方和最小化\n",
    "    $$E_{1}(x_q) \\equiv \\frac{1}{2} \\sum_{x \\in\\ k\\ nearest\\ nbrs\\ of\\ x_q} (f(x)- \\hat{f}(x))^2$$\n",
    "-   使整个训练样例集合D上的误差平方和最小化，但对每个训练样例加权，权值为关于相距xq距离的某个递减函数K：\n",
    "    $$E_{2}(x_q) \\equiv \\frac{1}{2} \\sum_{x \\in D} (f(x) - \\hat{f}(x))^2 K(d(x_{q}, x))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RBF Networks\n",
    "============\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Radial Basis Function Networks\n",
    "------------------------------\n",
    "\n",
    "-   全局逼近目标函数 , 是局部逼近的线性组合\n",
    "-   另一种神经网络\n",
    "-   与距离加权回归有密切联系，但属于积极（eager）方法，而不是消极（lazy）方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Radial Basis Function Networks\n",
    "------------------------------\n",
    "\n",
    "### 网络 \n",
    "\n",
    "![](./image/rbf2.png)\n",
    "\n",
    "### 说明 \n",
    "\n",
    "其中 $a_i(x)$ 是描述实例 $x$ 的属性 , 且有\n",
    "$$f(x) =  w_0 + \\sum_{u=1}^{k} w_u K_u(d(x_u,x))$$\n",
    "\n",
    "$K_u(d(x_u,x))$ 通常可选为：\n",
    "$$K_u(d(x_u,x)) = e^{- \\frac{1}{2 \\sigma_u^2}d^2(x_u,x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training Radial Basis Function Networks\n",
    "---------------------------------------\n",
    "\n",
    "-   Q1: 核函数 $K_u(d(x_u,x))$ 的 $x_u$ 如何选取？\n",
    "    -   均匀分布在实例空间中\n",
    "    -   或使用训练实例(反映了实例分布)\n",
    "-   Q2: 如何训练权重 (假设是 Gaussian $K_u$)\n",
    "    -   首先为每个$K_u$ 选择方差 (与均值)\n",
    "        -   例如，使用 EM 算法\n",
    "    -   然后固定 $K_u$ , 训练线性网络层\n",
    "        -   拟合线性函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Case-Based Reasoning\n",
    "====================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case-Based Reasoning\n",
    "--------------------\n",
    "\n",
    "当 $X \\neq \\Re^n$ 时 应用基于实例的学习 （需要不同的\"距离\"度量）\n",
    "\n",
    "基于案例的推理------基于实例的推理应用于符号逻辑描述\n",
    "\n",
    "```\n",
    "((user-complaint error53-on-shutdown)\n",
    " (cpu-model PowerPC)\n",
    " (operating-system Windows)\n",
    " (network-connection PCIA)\n",
    " (memory 48meg)\n",
    " (installed-applications Excel Netscape VirusScan)\n",
    " (disk 1gig)\n",
    " (likely-cause ???))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case-Based Reasoning in CADET\n",
    "-----------------------------\n",
    "\n",
    "-   CADET: 存储了 75 个机械设置样例\n",
    "    -   每个训练样例: $\\langle$ qualitative function,\n",
    "        mechanical-structure $\\rangle$\n",
    "    -   新查询: desired function,\n",
    "    -   目标: mechanical structure for this function\n",
    "-   距离度量: match qualitative function descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case-Based Reasoning in CADET\n",
    "-----------------------------\n",
    "\n",
    "![](./image/cbr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "与KNN区别\n",
    "---------\n",
    "\n",
    "基于案例的推理系统区别于k-近邻这样的方法的若干一般特征：\n",
    "\n",
    "-   实例或案例可以用丰富的符号描述表示，就像CADET中使用的功能图。\n",
    "    这可能需要不同于欧氏距离的相似性度量，比如两个功能图的最大可共享子图的大小。\n",
    "-   检索到的多个案例可以合并形成新问题的解决方案。\n",
    "    这与k-近邻方法相似------多个相似的案例用来构成对新查询的回答。\n",
    "    然而，合并多个检索到的案例的过程与k-近邻有很大不同，它依赖于知识推理而不是统计方法。\n",
    "-   案例检索、基于知识的推理和问题求解间是紧密耦合在一起的。\n",
    "    例如CADET系统在尝试找到匹配的案例过程中，它使用有关物理感应的一般知识重写了功能图。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lazy and Eager Learning\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lazy and Eager Learning\n",
    "-----------------------\n",
    "\n",
    "-   Lazy: 消极方法等到查询实例 $x_q$ 时从训练数据 $D$ 中泛化\n",
    "    -   $k$ -Nearest Neighbor, Case based reasoning\n",
    "-   Eager: 积极方法在见到查询实例 $x_q$\n",
    "    前，已经选进行了泛化（选取了对目标函数的（全局）逼近）。\n",
    "    -   Radial basis function networks, ID3, Backpropagation,\n",
    "        NaiveBayes, $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "积极学习的和消极学习之间的差异\n",
    "------------------------------\n",
    "\n",
    "-   积极学习的和消极学习之间的差异意味着对目标函数的全局逼近和局部逼近的差异。\n",
    "    -   消极的学习器可以通过很多局部逼近的组合（隐含地）表示目标函数，\n",
    "    -   积极的学习器必须在训练时提交单个的全局逼近。\n",
    "    -   对于同样的 $H$, 消极的学习器可表达更复杂的函数 (如 $H$ =\n",
    "        线性函数)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
