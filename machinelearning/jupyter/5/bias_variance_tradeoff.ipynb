{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bias--variance decomposition of squared error\n",
    "=============================================\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose $$y (x) = f (x) + \\varepsilon$$ where the noise $\\varepsilon$\n",
    "has zero mean and variance $\\sigma^2$. $\\hat{f}\n",
    "(x)$ is to approximate the true $f (x)$ by using information from\n",
    "$y (x)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For notational convenience, abbreviate $f = f (x), \\hat{f} = \\hat{f} (x), y = y (x)$. \n",
    "\n",
    "$$\\begin{align}\n",
    "  E (f - \\hat{f})^2 & =  E (f - E \\hat{f} + E \\hat{f} - \\hat{f})^2\\\\\n",
    "  & =  E (f - E \\hat{f})^2 + E (E \\hat{f} - \\hat{f})^2 + 2 E (f - E \\hat{f})\n",
    "  E (E \\hat{f} - \\hat{f})\\\\\n",
    "  & =  E (f - E \\hat{f})^2 + E (E \\hat{f} - \\hat{f})^2\\\\\n",
    "  & =  (\\operatorname{Bias} (\\hat{f}))^2 + \\operatorname{Var} (\\hat{f})\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Least square estimation\n",
    "=======================\n",
    "\n",
    "Linear regression problem can be represented as \n",
    "\n",
    "$$\\begin{aligned}\n",
    "  Y_{n \\times 1} & =  X_{n \\times n} \\theta_{n \\times 1} + \\varepsilon_{n\n",
    "  \\times 1}\\end{aligned}$$ \n",
    "  \n",
    "  where $\\varepsilon$ is noise with n row and 1\n",
    "column. $E (\\varepsilon\\varepsilon^T) = \\sigma^2 I$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The ordinary least square estimation of $\\theta$ is \n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\hat{\\theta} & =  \\arg \\min_{\\theta}  (Y - X \\theta)^T (Y - X \\theta)\\\\\n",
    "  & =  (X^T X)^{- 1} X^T Y\\\\\n",
    "  & =  (X^T X)^{- 1} X^T (X \\theta + \\varepsilon)\\\\\n",
    "  & =  \\theta + (X^T X)^{- 1} X^T \\varepsilon\\\\\n",
    "  E \\hat{\\theta} & =  \\theta\\\\\n",
    "  \\operatorname{Var} \\hat{\\theta} & =  (X^T X)^{- 1} X^T E (\\varepsilon \\varepsilon^T) X (X^T X)^{- 1}\\\\\n",
    "  & =  \\sigma^2 (X^T X)^{- 1}\\end{aligned}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The regularized least square estimation of $\\theta$ is \n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\hat{\\theta}_r & =  \\arg \\min_{\\theta} (Y - X \\theta)^T (Y - X \\theta) +\n",
    "  \\lambda \\theta^T \\theta\\\\\n",
    "  & =  (X^T X + \\lambda I)^{- 1} X^T Y\\\\\n",
    "  & =  (X^T X + \\lambda I)^{- 1} X^T (X \\theta + \\varepsilon)\\\\\n",
    "  & =  (X^T X + \\lambda I)^{- 1} X^T X \\theta + (X^T X + \\lambda I)^{- 1}  X^T \\varepsilon\\\\\n",
    "  E \\hat{\\theta}_r & =  (X^T X + \\lambda I)^{- 1} X^T X \\theta\\\\\n",
    "  \\operatorname{Var} \\hat{\\theta}_r & =  (X^T X + \\lambda I)^{- 1} X^T E (\\varepsilon\n",
    "  \\varepsilon^T) X (X^T X + \\lambda I)^{- 1}\\\\\n",
    "  & =  \\sigma^2 (X^T X + \\lambda I)^{- 1} X^T X (X^T X + \\lambda I)^{- 1}\\end{aligned}$$\n",
    "  \n",
    "When $\\lambda > 0$, there are $E \\hat{\\theta}_r > E \\hat{\\theta}, \\operatorname{Var} \\hat{\\theta}_r < \\operatorname{Var} \\hat{\\theta}$. It is a better choice to use regularized least square estimation in some cases\n",
    "to decrease squared estimating error $(\\theta - \\hat{\\theta})^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mean filtering\n",
    "==============\n",
    "\n",
    "Suppose there is a signal with random noise as \n",
    "$$\\begin{aligned}\n",
    "  y (n) & =  f (n) + \\varepsilon (n)\\end{aligned}$$\n",
    " \n",
    " $\\varepsilon (n)$\n",
    "is identical independence random variable with $E\n",
    "\\varepsilon (n) = 0, \\operatorname{Var} (\\varepsilon (n)) = \\sigma^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The value of signal $f (n)$ can be estimated by using mean filtering on\n",
    "$y\n",
    "(n)$ $$\\begin{aligned}\n",
    "  \\hat{f}_3 (n) & =  \\frac{1}{3} \\sum^1_{i = - 1} y (n + i)\\\\\n",
    "  E \\hat{f}_3 (n) & =  \\frac{1}{3} \\sum^1_{i = - 1} f (n + i)\\\\\n",
    "  \\operatorname{Var} \\hat{f}_3 (n) & =  \\frac{\\operatorname{Var} (\\varepsilon)}{3}\\\\\n",
    "  & =  \\frac{\\sigma^2}{3}\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " When using\n",
    "$\\hat{f}_1 (n) = y (n)$ to estimate $f (n)$ directly, $$\\begin{aligned}\n",
    "  E \\hat{f}_1 (n) & =  f (n)\\\\\n",
    "  \\operatorname{Var} (\\hat{f}_1 (n)) & =  \\sigma^2\\end{aligned}$$\n",
    "Bias is increased but variance is decreased in mean filtering."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
