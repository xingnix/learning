{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reverse-model",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 贝叶斯学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-thread",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-appraisal",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 贝叶斯学习方法的特性：\n",
    "\n",
    "-   观察到的每个训练样例可以增量式地降低或升高某假设的估计概率。这提供了一种比其他算法更合理的学习途径。其他算法会在某个假设与任一样例不一致时完全去掉该假设。\n",
    "-   先验知识可以与观察数据一起决定假设的最终概率。在贝叶斯学习中，先验知识的形式可以是 \n",
    "\n",
    "    - （1）每个候选假设的先验概率\n",
    "    - （2）每个可能假设在可观察数据上的概率分布。\n",
    "-   贝叶斯方法可允许假设做出不确定性的预测。（比如这样的假设：这一肺炎病人有\n",
    "    93% 的机会康复）。\n",
    "-   新的实例分类可由多个假设一起作出预测，以它们的概率为权重。\n",
    "-   即使在贝叶斯方法计算复杂度较高时，它们仍可做为一个最优的决策的标准衡量其他方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-louis",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 贝叶斯公式\n",
    "\n",
    "$$ P(h|D) = \\frac{P(D|h) P(h)}{P(D)} $$\n",
    "\n",
    "-   $P(h)$ = 还没有训练数据前，假设 $h$ 的概率。\n",
    "-   $P(D)$ = 训练数据D的先验概率\n",
    "-   $P(h|D)$ = $D$ 时 $h$ 成立的概率。\n",
    "-   $P(D|h)$ = 给定假设 $h$ 时观察到数据 $D$ 的概率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-manchester",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 最大后验 (Maximum a posteriori,MAP)\n",
    "\n",
    "$$ P(h|D) = \\frac{P(D|h) P(h)}{P(D)} $$\n",
    "\n",
    "通常，学习器考虑候选假设集合 $H$ 并在其中寻找给定数据 $D$\n",
    "时可能性最大的假设 $h\\in H$ 。\n",
    "\n",
    "这样的具有最大可能性的假设被称为极大后验（maximum a posteriori,\n",
    "MAP）假设：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "& h_{MAP} & = \\arg \\max_{h \\in H} P(h|D)\\nonumber \\\\\n",
    "& & = \\arg \\max_{h \\in H} \\frac{P(D|h) P(h)}{P(D)} \\nonumber \\\\\n",
    "& & = \\arg \\max_{h \\in H}P(D|h) P(h) \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-operator",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 极大似然（ Maximum likelihood，ML）\n",
    "\n",
    "假定 $P(h_{i})=P(h_{j})$ 则可进一步简化，选取极大似然假设：\n",
    "\n",
    "$$h_{ML}=\\arg \\max_{h \\in H}P(D|h)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-quantity",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 基本公式\n",
    "\n",
    "-   乘法公式(Product rule)：两事件A和B的交的概率 $P(A \\land B)$\n",
    "    $$P(A \\land B) = P(A|B) P(B) = P(B|A) P(A)$$\n",
    "-   加法公式(Sum Rule)：两事件A和B的并的概率 $P(A\\lor B)$\n",
    "    $$P(A \\lor B) = P(A) + P(B) - P(A \\land B)$$\n",
    "-   全概率公式(Theorem of total probability)：如果事件\n",
    "    $A_{1}, \\ldots, A_{n}$ 互斥且 $\\sum_{i = 1}^{n} P(A_{i}) = 1$ ，则：\n",
    "    $$P(B) = \\sum_{i = 1}^{n} P(B|A_{i}) P(A_{i})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-norfolk",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 贝叶斯学习 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-jacket",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 概念\n",
    "\n",
    "-   假定学习器考虑的是定义在实例空间 $X$ 上的有限的假设空间 $H$ ，\n",
    "-   任务是学习某个目标概念 $c:X\\rightarrow \\{0,1\\}$ 。\n",
    "-   为简化讨论，假定实例序列 $\\langle x_{1}, \\ldots, x_{m}\\rangle$\n",
    "    是固定不变的，\n",
    "-   训练数据 $D$ 可被简单地写作目标函数值序列：\n",
    "    $D = \\langle c(x_{1}),\\ldots, c(x_{m})\\rangle$ 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-headquarters",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brute-Force MAP学习算法\n",
    "\n",
    "-   对于 $H$ 中每个假设 $h$ ，计算后验概率：\n",
    "    $$P(h|D) = \\frac{P(D|h) P(h)}{P(D)}$$\n",
    "-   输出有最高后验概率的假设 $h_{MAP}$\n",
    "    $$h_{MAP} = \\arg \\max_{h \\in H} P(h|D)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-screw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brute-Force MAP学习算法（续）\n",
    "\n",
    "选择P(h)和P(D\\|h)的概率分布，以描述该学习任务的先验知识：\n",
    "\n",
    "-   训练数据 $D$ 是无噪声的（即 $d_i=c(x_i)$ ）；\n",
    "-   目标概念 $c$ 包含在假设空间H中；\n",
    "-   没有任何理由认为某假设比其他的假设的可能性大。\n",
    "-   选取 $P(D|h)$ :\n",
    "    -   $P(D|h)=1$ , 若 $h$ 与 $D$ 一致\n",
    "    -   $P(D|h)=0$ , 其它情况\n",
    "-   选取 $P(h)$ 服从均匀分布\n",
    "    -   $P(h) = \\frac{1}{|H|}$ , 对 $H$ 中的所有 $h$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-incident",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brute-Force MAP学习算法（续）\n",
    "\n",
    "可得：\n",
    "\n",
    "\\begin{align*}\n",
    "P(h|D) &=\\frac{P(D|h)P(h)}{P(D)}\\\\\n",
    "&=\\frac{P(D|h)P(h)}{\\sum_{h_i\\in H}P(D|h_i)P(h_i)}\\\\\n",
    "&=\\frac{P(D|h)\\cdot \\frac{1}{|H|}}{\\sum_{h_i\\in VS_{H,D}}1\\times \\frac{1}{|H|}+\\sum_{h_i\\not\\in VS_{H,D}}0\\times \\frac{1}{|H|}}\\\\\n",
    "&=\\frac{P(D|h)}{|VS_{H,D}|}\\\\\n",
    "&= \\left\\{ \\begin{array}{cl}\n",
    "  \\frac{1}{|VS_{H,D}|} & \\mbox{if $h$ is consistent with $D$} \\\\\n",
    "\\\\\n",
    "  0  & \\mbox{otherwise} \n",
    "\\end{array} \\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-forge",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 贝叶斯法则\n",
    "\n",
    "![](./image/bayes-vs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-strain",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 贝叶斯学习 \n",
    "\n",
    "![](./image/vs-map-equivalent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-desert",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MAP假设和一致学习器\n",
    "\n",
    "-   在给定条件下，与D一致的每个假设都是MAP假设。\n",
    "\n",
    "    -   根据这一结论可直接得到一类普遍的学习器，称为一致学习器。\n",
    "    -   某学习算法被称为一致学习器，说明它输出的假设在训练例上有零错误率。\n",
    "\n",
    "-   假定H上有均匀的先验概率（即 $P(h_i)=P(h_j)$ ，对所有的 $i,j$ ），\n",
    "\n",
    "-   且训练数据是确定性的和无噪声的 (即当D和h一致时， $P(D|h)=1$\n",
    "    ,否则为0）时，\n",
    "\n",
    "-   任意一致学习器将输出一个MAP假设。\n",
    "\n",
    "-   例如第2章讨论的Find-S概念学习算法:\n",
    "\n",
    "    -   Find-S按照特殊到一般的顺序搜索假设空间 $H$ ，\n",
    "    -   并输出一个极大特殊性的一致假设，\n",
    "    -   可知在上面定义的 $P(h)$ 和 $P(D|h)$ 概率分布下，它输出MAP假设。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-tutorial",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 极大似然\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-victor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 学习实值函数\n",
    "\n",
    "![](./image/bayes-linear.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-stanley",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 考虑实值函数 $f$\n",
    "\n",
    "-   训练样例 $\\langle x_{i}, d_{i} \\rangle$, 其中\n",
    "    $$d_{i} = f(x_{i}) + e_{i}$$\n",
    "    -   $e_{i}$ 是随机变量， 与 $x_{i}$ 独立， 服从零均值高斯分布\n",
    "-   最大似然估计 $h_{ML}$ ：\n",
    "    $$h_{ML} = \\arg \\min_{h \\in H} \\sum_{i=1}^{m} \\left(d_{i} -h(x_{i})\\right)^{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-worker",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 推导：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_{ML} &= &\\arg \\max_{h \\in H} p(D|h) \\nonumber \\\\\n",
    " &= &\\arg \\max_{h \\in H} \\prod_{i=1}^{m} p(d_{i}|h) \\nonumber \\\\\n",
    "&= &\\arg \\max_{h \\in H} \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\n",
    "e^{-\\frac{1}{2}(\\frac{d_{i} - h(x_{i})}{\\sigma})^{2}} \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-trout",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 用自然对数替换，得：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_{ML}  &= &\\arg \\max_{h \\in H}\n",
    "\\sum_{i=1}^{m} \\ln \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} -\n",
    "\\frac{1}{2}\\left(\\frac{d_{i} - h(x_{i})}{\\sigma}\\right)^{2} \\nonumber \\\\\n",
    "  &= &\\arg \\max_{h \\in H} \\sum_{i=1}^{m} -\n",
    "\\frac{1}{2}\\left(\\frac{d_{i} - h(x_{i})}{\\sigma}\\right)^{2} \\nonumber \\\\\n",
    " &= &\\arg \\max_{h \\in H} \\sum_{i=1}^{m} - \\left(d_{i} - h(x_{i})\\right)^{2}\n",
    " \\nonumber \\\\\n",
    " &= &\\arg \\min_{h \\in H} \\sum_{i=1}^{m} \\left(d_{i} - h(x_{i})\\right)^{2}  \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-regard",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 用于预测概率的极大似然假设\n",
    "\n",
    "考虑从数据中预测概率\n",
    "\n",
    "-   训练样例 $\\langle x_{i}, d_{i} \\rangle$, 其中 $d_{i}$ 为 1 或 0\n",
    "\n",
    "-   训练神经网络根据给定的 $x_i$ 输出一个概率\n",
    "\n",
    "    \\begin{align*}\n",
    "       h_{ML} &= \\arg \\max_{h \\in H} \\prod_{i=1}^{m} h(x_{i})^{d_i} (1 - h(x_{i}))^{1-d_{i}}\\\\\n",
    "              &= \\arg \\max_{h \\in H} \\sum_{i=1}^{m} d_{i} \\ln h(x_{i}) + (1-d_{i})\\ln (1 - h(x_{i}))\n",
    "    \\end{align*}\n",
    "\n",
    "-   sigmoid单元的权值更新: $$w_{jk} \\leftarrow w_{jk} +  \\Delta w_{jk}$$\n",
    "    其中：\n",
    "    $$\\Delta w_{jk} = \\eta \\sum_{i=1}^{m} (d_{i} - h(x_{i})) \\  x_{ijk}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-iraqi",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 最小描述长度准则\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-origin",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 奥坎坶剃刀 （Occam\\'s razor）\n",
    "\n",
    "-   \"为观察到的数据选择最短的解释\"。（优先选择短的假设）\n",
    "-   最小描述长度准则（Minimum Description Length, MDL）:\n",
    "    -   优先选择最小化\n",
    "        $$h_{MDL} = \\arg \\min_{h \\in H} L_{C_{1}}(h) + L_{C_{2}}(D|h)$$\n",
    "        的假设 $h$\n",
    "    -   其中 $L_{C}(x)$ 是在编码 $C$ 下 $x$ 的描述长度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-court",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 示例：\n",
    "\n",
    "-   $H$ = 决策树\n",
    "-   $D$ = 训练数据\n",
    "-   $L_{C_{1}}(h)$ 是 $h$ 的编码长度\n",
    "-   $L_{C_{2}}(D|h)$ 给定 $h$ 时， $D$ 的编码长度\n",
    "-   当样例被 $h$ 完美分类时， $L_{C_{2}}(D|h)=0$\n",
    "-   $h_{MDL}$ 考虑了树的大小与训练误差\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_{MAP} &= &\\arg \\max_{h \\in H}P(D|h) P(h) \\nonumber \\\\\n",
    "&= &\\arg \\max_{h \\in H} \\log_{2} P(D|h) + \\log_{2} P(h)  \\nonumber \\\\\n",
    "&= &\\arg \\min_{h \\in H} - \\log_{2} P(D|h) - \\log_{2} P(h) \n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-cannon",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 由信息论可得：\n",
    "\n",
    "针对以概率 $p$ 发生的事件，最优 (最短期望编码长度)编码是 $- \\log_{2} p$ 位.\n",
    "\n",
    "\n",
    "-   $- \\log_{2} P(h)$ 是 $h$ 的最优编码长度\n",
    "-   $- \\log_{2} P(D|h)$ 是给定 $h$ 后 $D$ 的最优编码长度\n",
    "\n",
    "$\\rightarrow$ 优先选择最小化 $$length(h) + length(misclassifications)$$\n",
    "的假设\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-dictionary",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 贝叶斯最优分类器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-permit",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 新实例的最大可能分类\n",
    "\n",
    "-   给定训练数据 $D$ ,最可能的假设是什么?( $h_{MAP}$)\n",
    "-   给定训练数据 $D$ ,对新实例 $x$ 的最可能分类是什么？\n",
    "\n",
    "考虑三个假设：\n",
    "\n",
    "-   $P(h_{1}|D)=.4, \\  P(h_{2}|D)=.3, \\  P(h_{3}|D)=.3$\n",
    "\n",
    "对于新的实例 $x$,\n",
    "\n",
    "-   $h_{1}(x)=+, \\ h_{2}(x)=-, \\ h_{3}(x)=-$\n",
    "-   $x$ 的最大可能分类是什么?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-advice",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 贝叶斯最优分类器\n",
    "\n",
    "$$\\arg \\max_{v_{j} \\in V} \\sum_{h_{i} \\in H} P(v_{j}|h_{i}) P(h_{i}|D)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-distance",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 示例:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "P(h_{1}|D)=.4, & P(-|h_{1})=0, & P(+|h_{1})=1 \\nonumber \\\\\n",
    "P(h_{2}|D)=.3, & P(-|h_{2})=1, & P(+|h_{2})=0 \\nonumber \\\\\n",
    "P(h_{3}|D)=.3, & P(-|h_{3})=1, & P(+|h_{3})=0 \\nonumber \n",
    "\\end{eqnarray}\n",
    "\n",
    "因此\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_{h_{i} \\in H} P(+|h_{i}) P(h_{i}|D) & = & .4 \\nonumber \\\\\n",
    "\\sum_{h_{i} \\in H} P(-|h_{i}) P(h_{i}|D) & = & .6 \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "与\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\arg \\max_{v_{j} \\in V} \\sum_{h_{i} \\in H} P(v_{j}|h_{i}) P(h_{i}|D) & = & -\n",
    "\\nonumber \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-revolution",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GIBBS 算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-mainland",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GIBBS 算法\n",
    "\n",
    "-   虽然贝叶斯最优分类器能从给定训练数据中获得最好的性能，应用此算法的开销可能很大。\n",
    "\n",
    "-   原因在于它要计算H中每个假设的后验概率，然后合并每个假设的预测，以分类新实例。\n",
    "\n",
    "-   一个替代的、非最优的方法是Gibbs算法，定义如下：\n",
    "\n",
    "    当有一待分类新实例时，Gibbs算法简单地按照当前的后验概率分布，使用一随机抽取的假设。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-discretion",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gibbs算法:\n",
    "\n",
    "-   按照 $H$ 上的后验概率分布 $P(h|D)$ ，从 $H$ 中随机选择假设 $h$ 。\n",
    "\n",
    "-   使用h来预言下一实例x的分类。\n",
    "\n",
    "-   可证明在一定条件下Gibbs算法的误分类率的期望值最多为贝叶斯最优分类器的两倍。\n",
    "\n",
    "-   更精确地讲，期望值是在随机抽取的目标概念上作出，抽取过程按照学习器假定的先验概率。\n",
    "\n",
    "-   在此条件下，Gibbs算法的错误率期望值最差为贝叶斯分类器的两倍。\n",
    "    $$ E[error_{Gibbs}] \\leq 2 E[error_{Bayes Optimal}] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-irish",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 概念学习问题分析：\n",
    "\n",
    "-   如果学习器假定 $H$\n",
    "    上有均匀的先验概率，而且如果目标概念实际上也按该分布抽取\n",
    "-   那么当前变型空间中随机抽取的假设对下一实例分类的期望误差最多为贝叶斯分类器的两倍。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-marathon",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 朴素贝叶斯分类器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-native",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 朴素贝叶斯分类器（ Naive Bayes Classifier ）\n",
    "\n",
    "-   贝叶斯学习方法中实用性很高的一种为朴素贝叶斯学习器，常被称为朴素贝叶斯分类器（naive\n",
    "    Bayes classifier）。在某些领域内其性能可与神经网络和决策树学习相当。\n",
    "    -   何时使用：\n",
    "        -   中等或大训练集\n",
    "        -   描述实例的属性在给定类别后条件独立\n",
    "-   已成功应用于\n",
    "    -   诊断\n",
    "    -   文本分类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-former",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 描述\n",
    "\n",
    "-   假定目标函数 $f: X \\rightarrow V$, 其中每个实例 $x$ 由属性\n",
    "    $\\langle a_{1}, a_{2} \\ldots a_{n} \\rangle$ 描述.\n",
    "\n",
    "-   $f(x)$ 的最大可能值为:\n",
    "\n",
    "    \\begin{eqnarray}\n",
    "        v_{MAP} &= &\\arg \\max_{v_{j} \\in V} P(v_{j} | a_{1}, a_{2} \\ldots a_{n})  \\nonumber \\\\ \n",
    "        v_{MAP} &= &\\arg \\max_{v_{j} \\in V} \\frac{P(a_{1}, a_{2} \\ldots a_{n}|v_{j})\n",
    "        P(v_{j})}{P(a_{1}, a_{2} \\ldots a_{n})} \\nonumber \\\\ \n",
    "        &= &\\arg \\max_{v_{j} \\in V} P(a_{1}, a_{2} \\ldots a_{n}|v_{j}) P(v_{j}) \\nonumber\n",
    "    \\end{eqnarray}\n",
    "\n",
    "-   Naive Bayes 假定:\n",
    "    $$ P(a_{1}, a_{2} \\ldots a_{n}|v_{j}) = \\prod_{i} P(a_{i} | v_{j}) $$\n",
    "\n",
    "-   可得：\n",
    "    $$\\mbox{ Naive Bayes classifier: } v_{NB} = \\arg \\max_{v_{j} \\in V} P(v_{j})\\prod_{i} P(a_{i} | v_{j}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-trade",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 算法\n",
    "\n",
    "-   Naive_Bayes_Learn($examples$) 对每个目标值 $v_j$\n",
    "    -   $\\hat{P}(v_j) \\leftarrow$ 估计 $P(v_j)$\n",
    "    -   对每个属性 $a$ 的 每个可能 取值 $a_i$\n",
    "        -   $\\hat{P}(a_i|v_j) \\leftarrow$ 估计 $P(a_i|v_j)$\n",
    "-   Classify_New_Instance($x$)\n",
    "    $$v_{NB} = \\arg \\max_{v_{j} \\in V} \\hat{P}(v_{j}) \\prod_{a_i \\in x} \\hat{P}(a_{i} | v_{j})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-visit",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 示例\n",
    "\n",
    "-   PlayTennis中, 新实例：\n",
    "    $$\\langle Outlk=sun, Temp=cool, Humid=high, Wind=strong \\rangle$$\n",
    "-   不同目标值的概率可以基于这14个训练样例的频率很容易地估计出：\n",
    "    -   $P(PlayTennis=yes)=9/14=0.64$\n",
    "    -   $P(PlayTennis=no)=5/14=0.36$\n",
    "-   相似地，可以估计出条件概率，例如对于Wind=Strong有：\n",
    "    -   $P(Wind=strong|PlayTennis=yes)=3/9=0.33$\n",
    "    -   $P(Wind=strong|PlayTennis=no)=3/5=0.60$\n",
    "-   计算:\n",
    "    $$v_{NB} = \\arg \\max_{v_{j} \\in V} P(v_{j}) \\prod_{i} P(a_{i} | v_{j})$$\n",
    "    $$P(y)\\ P(sun|y)\\ P(cool|y)\\ P(high|y)\\ P(strong|y) = .005 $$\n",
    "    $$P(n)\\ P(sun|n)\\ P(cool|n)\\ P(high|n)\\ P(strong|n) = .021 $$\n",
    "    $$ \\rightarrow v_{NB} = n $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-sheet",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Naive Bayes: Subtleties\n",
    "\n",
    "-   通常不满足独立性假定\n",
    "    $$P(a_{1}, a_{2} \\ldots a_{n}|v_{j}) = \\prod_{i} P(a_{i} | v_{j})$$\n",
    "-   但还是会有很好的表现。注意：不需要估计到的后验概率 $\\hat{P}(v_j|x)$\n",
    "    是正确的，只需要：\n",
    "    $$\\arg\\max_{v_{j}\\in V}\\hat{P}(v_{j})\\prod_{i}\\hat{P}(a_{i}|v_{j})=\\arg\\max_{v_{j} \\in V}  P(v_{j}) P(a_{1} \\ldots, a_n | v_{j})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-debate",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 当目标值为 $v_j$ 的所有训练实例都没有属性值 $a_i$? 时\n",
    "\n",
    "$$\\hat{P}(a_i|v_j) = 0 \\mbox{, and...}$$\n",
    "$$\\hat{P}(v_{j}) \\prod_{i} \\hat{P}(a_{i} | v_{j}) = 0$$\n",
    "典型的解决方法是对 $\\hat{P}(a_{i} | v_{j})$ 进行贝叶斯估计\n",
    "$$\\hat{P}(a_{i} | v_{j}) \\leftarrow \\frac{n_{c} + mp}{n + m}$$ 其中：\n",
    "\n",
    "-   $n$ 是 $v=v_j$ 的训练样例的数量\n",
    "-   $n_c$ 是 $v=v_j$ 且 $a=a_i$ 的样例数量\n",
    "-   $p$ 是对 $\\hat{P}(a_{i} | v_{j})$ 的先验估计\n",
    "-   $m$ 是对先验的权重 (等效样本大小)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-guitar",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 学习分类文本\n",
    "\n",
    "-   学习将文本按兴趣分类\n",
    "-   学习将网页按主题分类\n",
    "\n",
    "目标概念： $Interesting? : Document \\rightarrow \\{+,-\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-canada",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 将文档表示为单词向量\n",
    "\n",
    "-   one attribute per word position in document\n",
    "-   Learning: Use training examples to estimate\n",
    "    -   $P(+)$\n",
    "    -   $P(-)$\n",
    "    -   $P(doc|+)$\n",
    "    -   $P(doc|-)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-advance",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 朴素贝叶斯条件独立假定\n",
    "\n",
    "$$P(doc|v_j) = \\prod_{i=1}^{length(doc)} P(a_i=w_k | v_j)$$\n",
    "\n",
    "其中 $P(a_i=w_k| v_j)$ 是给定 $v_j$ 时， 位置 $i$ 的单词是 $w_k$\n",
    "的概率。\n",
    "\n",
    "另一假定: $P(a_i=w_k|v_j) = P(a_m=w_k|v_j), \\forall i,m$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-berry",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 算法流程\n",
    "\n",
    "Learn_naive_Bayes_text( $Examples, V$ )\n",
    "\n",
    "-   Examples为一组文本文档以及它们的目标值。\n",
    "-   V为所有可能目标值的集合。\n",
    "-   此函数作用是学习概率项 $P(w_k|v_j)$ ，\n",
    "-   它描述了从类别 $v_j$ 中的一个文档中随机抽取的一个单词为英文单词\n",
    "    $w_k$ 的概率。该函数也学习类别的先验概率 $P(v_j)$ 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-wildlife",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 算法流程(续)\n",
    "\n",
    "-   收集Examples中所有的单词、标点符号以及其他记号\n",
    "    -   $Vocabulary \\leftarrow$\n",
    "        在Examples中任意文本文档中出现的所有单词及记号的集合\n",
    "-   计算所需要的概率项 $P(v_j)$ 和 $P(w_k|v_j)$\n",
    "    -   对V中每个目标值 $v_j$\n",
    "        -   $docs_{j} \\leftarrow$ Examples中目标值为 $v_j$ 的文档子集\n",
    "        -   $P(v_{j}) \\leftarrow \\frac{|docs_{j}|}{|Examples|}$\n",
    "        -   $Text_{j} \\leftarrow$ 将 $docs_j$\n",
    "            中所有成员连接起来建立的单个文档\n",
    "        -   $n \\leftarrow$ 在 $Text_j$\n",
    "            中不同单词位置的总数(重复单词多次计算)\n",
    "        -   对 $Vocabulary$ 中每个单词 $w_k$\n",
    "            -   $n_{k} \\leftarrow$ 单词 $w_k$ 出现在 $Text_j$ 中的次数\n",
    "            -   $P(w_{k}|v_{j}) \\leftarrow \\frac{n_{k} + 1}{n + |Vocabulary|}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-opera",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 算法流程(续)\n",
    "\n",
    "Classify_naive_Bayes_text($Doc$)\n",
    "\n",
    "-   对文档 $Doc$ 返回其估计的目标值。$a_i$ 代表在 $Doc$ 中的第 $i$\n",
    "    个位置上出现的单词。\n",
    "    -   $positions \\leftarrow$ 在 $Doc$ 中包含的能在 $Vocabulary$\n",
    "        中找到的记号的所有单词位置\n",
    "    -   返回\n",
    "        $$v_{NB} = \\arg\\max_{v_{j} \\in V} P(v_{j}) \\prod_{i \\in positions}P(a_{i}|v_{j})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-picnic",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Twenty NewsGroups\n",
    "\n",
    "\n",
    "Given 1000 training documents from each group\n",
    "\n",
    "Learn to classify new documents according to which newsgroup it came\n",
    "from\n",
    "\n",
    "|                           |                          |\n",
    "|---------------------------|--------------------------|\n",
    "|comp.graphics              |  misc.forsale            |\n",
    "|comp.os.ms-windows.misc    | rec.autos                |\n",
    "|comp.sys.ibm.pc.hardware   | rec.motorcycles |\n",
    "|comp.sys.mac.hardware      | rec.sport.baseball |\n",
    "|comp.windows.x             | rec.sport.hockey |\n",
    "|                              |             |\n",
    "|alt.atheism                | sci.space \n",
    "|soc.religion.christian     | sci.crypt\n",
    "|talk.religion.misc         | sci.electronics\n",
    "|talk.politics.mideast      | sci.med \n",
    "|talk.politics.misc         | \n",
    "|talk.politics.guns         |  \n",
    "\n",
    "Naive Bayes: 89% classification accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-original",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Article from rec.sport.hockey\n",
    "\n",
    "\n",
    "``` example\n",
    "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!ogicse!uwm.edu\n",
    "From: xxx@yyy.zzz.edu (John Doe)\n",
    "Subject: Re: This year's biggest and worst (opinion)...\n",
    "Date: 5 Apr 93 09:53:39 GMT\n",
    "\n",
    "I can only comment on the Kings, but the most \n",
    "obvious candidate for pleasant surprise is Alex\n",
    "Zhitnik. He came highly touted as a defensive \n",
    "defenseman, but he's clearly much more than that. \n",
    "Great skater and hard shot (though wish he were \n",
    "more accurate). In fact, he pretty much allowed \n",
    "the Kings to trade away that huge defensive \n",
    "liability Paul Coffey. Kelly Hrudey is only the \n",
    "biggest disappointment if you thought he was any \n",
    "good to begin with. But, at best, he's only a \n",
    "mediocre goaltender. A better choice would be \n",
    "Tomas Sandstrom, though not through any fault of \n",
    "his own, but because some thugs in Toronto decided \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-armor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Curve for 20 Newsgroups\n",
    "\n",
    "Accuracy vs. Training set size (1/3 withheld for test)\n",
    "![](./image/bayes-text-results.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-tomato",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 贝叶斯信念网\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-intention",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 贝叶斯信念网(Bayesian Belief Networks)\n",
    "\n",
    "-   贝叶斯置信网描述的是一组变量所遵从的概率分布，它通过一组条件概率来指定一组条件独立性假定。\n",
    "-   朴素贝叶斯分类器假定所有变量在给定目标变量值时为条件独立的，与此不同，贝叶斯置信网中可表述应用到变量的一个子集上的条件独立性假定。\n",
    "-   因此，贝叶斯置信网提供了一种中间的方法，它比朴素贝叶斯分类器中条件独立性的全局假定的限制更少，又比在所有变量中计算条件依赖更可行。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-correlation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 条件独立\n",
    "\n",
    "\n",
    "定义: 若给定 $Z$ 的值， $X$ 的概率分布独立于 $Y$的值，即：\n",
    "\n",
    "$$(\\forall x_i,y_j,z_k) \\ P(X = x_i | Y = y_j, Z = z_k) =   P(X = x_i | Z = z_k)$$\n",
    "\n",
    "则称 $X$ 在给定 $Z$ 时条件独立于 $Y$ . 记作：\n",
    "$$P(X | Y,Z) = P(X | Z)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-concert",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 示例:\n",
    "\n",
    "给定 $Lightning$ 则 $Thunder$ 条件独立于 $Rain$,\n",
    "$$P(Thunder | Rain, Lightning) = P(Thunder | Lightning)$$\n",
    "\n",
    "Naive Bayes 推导中使用了条件独立：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "P(X,Y|Z) &= &P(X|Y,Z) P(Y|Z)  \\nonumber \\\\\n",
    " &= &P(X|Z) P(Y|Z)  \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-table",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 网络\n",
    "\n",
    "![](./image/bayesnet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-pledge",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 说明\n",
    "\n",
    "-   贝叶斯网表示联合概率分布的方法是指定一组条件独立性假定（有向无环图），以及一组局部条件概率集合。\n",
    "\n",
    "-   联合空间中每个变量在贝叶斯网中表示为一结点。\n",
    "\n",
    "-   对每一变量需要两种类型的信息。首先，网络弧表示断言\"此变量在给定其立即前驱时条件独立于其非后继\"。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-hearing",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 表示联合概率 ：\n",
    "\n",
    "-   例如： $P(Storm, BusTourGroup, \\ldots, ForestFire)$\n",
    "-   对网络变量的元组 $(Y_1, \\ldots, Y_n)$ 取值 $(y_1, \\ldots, y_n)$\n",
    "    的联合概率：\n",
    "    $$P(y_1, \\ldots, y_n) = \\prod_{i=1}^{n} P(y_i | Parents(Y_i))$$\n",
    "\n",
    "其中 $Parents(Y_i)$ 表示网络中 $Y_i$ 的立即前驱的集合。注意\n",
    "$P(y_i|Parents(Y_i))$ 的值等于与结点 $Y_i$ 关联的条件概率表中的值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-subscription",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 贝叶斯网络推理\n",
    "\n",
    "-   可以用贝叶斯网在给定其他变量的观察值时推理出某些目标变量（如ForestFire）的值。\n",
    "-   由于所处理的是随机变量，所以一般不会赋予目标变量一个确切的值。\n",
    "-   真正需要推理的是目标变量的概率分布，它指定了在给与其他变量的观察值条件下，目标变量取每一可能值的概率。\n",
    "-   在网络中所有其他变量都确切知道了以后，这一推理步骤是很简单的。\n",
    "-   在更通常的情况下,我们希望在知道一部分变量的值（比如Thunder\n",
    "    和BusTourGroup为仅有可用的观察值）时获得某变量的概率分布（如ForestFire）。\n",
    "-   一般地，贝叶斯网络可用于在知道某些变量的值或分布时计算网络中另一部分变量的概率分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-batch",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 学习贝叶斯网络\n",
    "\n",
    "-   网络结构预先给出，或可由训练数据中推得。\n",
    "-   所有的网络变量可以直接从每个训练样例中观察到，或某些变量不能观察到。\n",
    "-   在网络结构的预先已知，并且变量可以从训练样例中完全获得时，通过学习得到条件概率表就比较简单了。只需要象在朴素贝叶斯分类器中那样估计表中的条件概率项。\n",
    "\n",
    "若网络结构已知，但只有一部分变量值能在数据中观察到。\n",
    "\n",
    "-   这一问题在某种程度上类似于在人工神经网络中学习隐藏单元的权值，其中输入和输出结点值由训练样例给出，但隐藏单元的值未指定。\n",
    "-   梯度上升过程可以学习条件概率表中的项。梯度上升过程搜索一个假设空间，它对应于条件概率表中所有可能的项。\n",
    "-   在梯度上升中最大化的目标函数是给定假设 $h$ 下观察到训练数据 $D$\n",
    "    的概率 $P(D|h)$ 。按照定义，它对应于对表项搜索极大似然假设。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-companion",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 梯度上升算法\n",
    "\n",
    "-   令 $w_{ijk}$ 代表一个条件概率表的一个表项。确切地讲，令 $w_{ijk}$\n",
    "    为在给定父结点 $U_i$ 取值 $u_{ik}$ 时，网络变量 $Y_i$ 值为 $y_{ij}$\n",
    "    的概率。\n",
    "    $$w_{ijk} = P(Y_i=y_{ij} | Parents(Y_i) = \\mbox{the list $u_{ik}$ of values)}$$\n",
    "\n",
    "    若 $Y_i = Campfire$ 则 $u_{ik}$ 可能是\n",
    "    $\\langle Storm=T, BusTourGroup=F \\rangle$ 例如，若 $w_{ijk}$\n",
    "    为图中条件概率表中最右上方的表项，那么 $Y_i$ 为变量 $Campfire$ ，\n",
    "    $U_i$ 是其父结点的元组 $<Storm, BusTourGroup>$ ， $y_{ij}=True$ ，\n",
    "    并且 $u_{ik}=<False, False>$ 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-configuration",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 梯度上升算法(续)\n",
    "\n",
    "-   通过 $\\ln P(D|h)$ 的梯度来使 $P(D|h)$ 最大化。\n",
    "-   重复执行梯度上升\n",
    "    -   使用训练数据 $D$ 更新所有 $w_{ijk}$\n",
    "        $$w_{ijk} \\leftarrow w_{ijk} + \\eta \\sum_{d \\in D} \\frac{P_h(y_{ij}, u_{ik} |d)}{w_{ijk}}$$\n",
    "    -   重新归一化 $w_{ijk}$ ， 保证\n",
    "        -   $\\sum_{j} w_{ijk} = 1$\n",
    "        -   $0 \\leq w_{ijk} \\leq 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-registrar",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More on Learning Bayes Nets\n",
    "\n",
    "可使用 EM 算法\n",
    "\n",
    "-   假定 $h$ 计算未观测到的变量概率\n",
    "-   计算新的 $w_{ijk}$ 最大化 $E[\\ln P(D|h)]$ ，其中 $D$\n",
    "    已包含观测到的与未观测到（但计算出了概率）的变量\n",
    "\n",
    "当结构未知时\n",
    "\n",
    "-   可使用贪婪搜索增/删结点与边\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-adaptation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-default",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation Maximization (EM)\n",
    "\n",
    "-   观测到部分数据\n",
    "-   实例的部分属性未知\n",
    "-   无监督聚类\n",
    "-   训练 Bayesian Belief Networks\n",
    "-   学习 Hidden Markov Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-observation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating Data from Mixture of $k$ Gaussians\n",
    "\n",
    "![](./image/two-gaussians.png)\n",
    "\n",
    "每个实例 $x$ 按如下方式产生：\n",
    "\n",
    "-   按均匀分布选取 $k$ 个高斯分布之一\n",
    "-   按此高斯分布随机产生一个实例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-ceremony",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EM for Estimating $k$ Means\n",
    "\n",
    "已知:\n",
    "\n",
    "-   从 $k$ 个高斯分布产生的实例 $x$\n",
    "-   $k$ 个高斯 分布的 均值 $\\langle \\mu_1, \\ldots, \\mu_k \\rangle$ 未知\n",
    "-   不知实例 $x_i$ 从哪个高斯分布产生\n",
    "\n",
    "求解:\n",
    "\n",
    "-   $\\langle \\mu_1, \\ldots, \\mu_k \\rangle$ 的最大似然估计\n",
    "\n",
    "将实例完整描述为 $y_i = \\langle x_i, z_{i1}, z_{i2}\\rangle$, 其中\n",
    "\n",
    "-   $z_{ij}$ 为 1 ，当 $x_i$ 由第 $j$ 个高斯分布产生\n",
    "-   $x_i$ 可观测\n",
    "-   $z_{ij}$ 不可观测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-transition",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EM Algorithm:\n",
    "\n",
    "随机选取初始值 $h = \\langle \\mu_1, \\mu_2 \\rangle$, 然后迭代：\n",
    "\n",
    "-   E step:\n",
    "    -   计算每个隐藏变量 $z_{ij}$ 的期望值 $E[z_{ij}]$ ，假定当前假设\n",
    "        $h = \\langle \\mu_1, \\mu_2 \\rangle$ 成立\n",
    "\n",
    "      \n",
    "  \\begin{eqnarray}\n",
    "     E[z_{ij}] & = & \\frac{p(x=x_i | \\mu = \\mu_j)}{\\sum_{n=1}^{2} p(x = x_i | \\mu=\\mu_n)} \\nonumber \\\\\n",
    "         & = & \\frac{e^{-\\frac{1}{2 \\sigma^2} (x_i -  \\mu_j)^2}}{\\sum_{n=1}^{2} e^{-\\frac{1}{2 \\sigma^2} (x_i - \\mu_n)^2}} \\nonumber\n",
    "        \\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-chaos",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EM Algorithm:\n",
    "\n",
    "-   M step:\n",
    "    -   计算一个新的极大似然假设 $h' = \\langle \\mu_1', \\mu_2' \\rangle$\n",
    "        ，\n",
    "    -   假定由每个隐藏变量 $z_{ij}$ 所取的值为 E step 中得到的期望值\n",
    "        $E[z_{ij}]$ ，\n",
    "    -   然后将假设 $h =\\langle \\mu_1, \\mu_2 \\rangle$ 替换为新的假设\n",
    "        $h' = \\langle \\mu_1', \\mu_2' \\rangle$ ，\n",
    "        $$\\mu_j \\leftarrow \\frac{\\sum_{i=1}^m E[z_{ij}] \\ \\  x_i}{\\sum_{i=1}^m E[z_{ij}]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-multiple",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EM Algorithm\n",
    "\n",
    "-   Converges to local maximum likelihood $h$\n",
    "-   and provides estimates of hidden variables $z_{ij}$\n",
    "-   In fact, local maximum in $E[\\ln P(Y|h)]$\n",
    "    -   $Y$ is complete (observable plus unobservable variables) data\n",
    "    -   Expected value is taken over possible values of unobserved\n",
    "        variables in $Y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-result",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General EM Problem\n",
    "\n",
    "已知:\n",
    "\n",
    "-   观测数据 $X=\\{x_1, \\ldots, x_m\\}$\n",
    "-   未观测数据 $Z=\\{z_1, \\dots, z_m\\}$\n",
    "-   参数化概率分布 $P(Y|h)$, 其中 $Y=\\{y_1, \\dots, y_m\\}$ 是数据\n",
    "    $y_i = x_i \\cup z_i$ ， $h$ 是参数\n",
    "\n",
    "求解:\n",
    "\n",
    "-   (局部)最大化 $E[\\ln P(Y|h)]$ 的 $h$\n",
    "\n",
    "用于:\n",
    "\n",
    "-   Train Bayesian belief networks\n",
    "-   Unsupervised clustering (e.g., $k$ means)\n",
    "-   Hidden Markov Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-payday",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General EM Problem\n",
    "\n",
    "定义似然函数 $Q(h' | h)$ ， 使用观测到的 $X$ 与当前参数 $h$ 估计 $Z$,\n",
    "计算 $Y = X \\cup Z$ $$Q(h' | h) \\leftarrow E[ \\ln P(Y | h') | h, X ]$$\n",
    "\n",
    "EM Algorithm:\n",
    "\n",
    "-   Estimation (E) step: 使用当前假设 $h$ 和观察到的数据 $X$ 来估计 $Y$\n",
    "    上的概率分布以计算 $Q(h'|h)$ 。\n",
    "    $$Q(h' | h) \\leftarrow E[ \\ln P(Y | h') | h, X ]$$\n",
    "-   Maximization (M) step:} 将假设 $h$ 替换为使 $Q$ 函数最大化的假设\n",
    "    $h'$ ： $$h \\leftarrow \\arg \\max_{h'}  Q(h' | h)$$``"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
